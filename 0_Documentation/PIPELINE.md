# DocumentaciÃ³n TÃ©cnica del Pipeline Python

## ğŸ“‹ Tabla de Contenidos

- [InformaciÃ³n General](#-informaciÃ³n-general)
- [Arquitectura Modular](#-arquitectura-modular)
- [Flujo de Procesamiento](#-flujo-de-procesamiento)
- [MÃ³dulos del Pipeline](#-mÃ³dulos-del-pipeline)
- [Funciones Principales](#-funciones-principales)
- [Formatos de Datos](#-formatos-de-datos)
- [ValidaciÃ³n y Control de Calidad](#-validaciÃ³n-y-control-de-calidad)
- [ConfiguraciÃ³n Avanzada](#-configuraciÃ³n-avanzada)
- [OptimizaciÃ³n y Rendimiento](#-optimizaciÃ³n-y-rendimiento)
- [ResoluciÃ³n de Problemas](#-resoluciÃ³n-de-problemas)

---

## ğŸ“Œ InformaciÃ³n General

**Archivo principal:** `3_processing_pipeline/process_RAW_data_WI.py`  
**VersiÃ³n:** 3.0 (Arquitectura Modular)  
**Autor:** Proyecto OTUS - Instituto Humboldt  
**Ãšltima actualizaciÃ³n:** Enero 2025

### PropÃ³sito

Pipeline ETL (Extract, Transform, Load) que procesa datos crudos de cÃ¡maras trampa desde Wildlife Insights y genera archivos Parquet optimizados para visualizaciÃ³n en dashboards R Shiny.

### Capacidades

- âœ… **Carga masiva** de archivos CSV (mÃºltiples proyectos)
- âœ… **ValidaciÃ³n automÃ¡tica** de formato y calidad
- âœ… **Filtrado inteligente** por eventos de muestreo (YYYY_N)
- âœ… **Enriquecimiento** de taxonomÃ­a y metadata
- âœ… **AnÃ¡lisis geoespacial** (asignaciÃ³n de CARs por coordenadas)
- âœ… **GeneraciÃ³n optimizada** de Parquet (columnar compression)
- âœ… **Reportes de calidad** detallados

---

## ğŸ—ï¸ Arquitectura Modular

### Estructura de Directorios

```
3_processing_pipeline/
â”œâ”€â”€ process_RAW_data_WI.py       # Orquestador principal (389 lÃ­neas)
â”œâ”€â”€ requirements.txt             # Dependencias Python
â””â”€â”€ src/                         # MÃ³dulos del pipeline
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ utils.py                 # Carga y filtrado de datos
    â”œâ”€â”€ transformations.py       # Transformaciones y enriquecimiento
    â”œâ”€â”€ generate_parquets.py     # GeneraciÃ³n de archivos Parquet
    â””â”€â”€ validation.py            # ValidaciÃ³n de calidad
```

### Diagrama de Dependencias

```
process_RAW_data_WI.py (main)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     src/                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                   â”‚
â”‚  utils.py                                         â”‚
â”‚    â”œâ”€ concatenar_archivos_csv()                   â”‚
â”‚    â”œâ”€ procesar_timestamps()                       â”‚
â”‚    â”œâ”€ filtrar_por_subproject_valido()             â”‚
â”‚    â””â”€ limpiar_registros_cv()                      â”‚
â”‚                                                   â”‚
â”‚  transformations.py                               â”‚
â”‚    â”œâ”€ crear_nombre_cientifico()                   â”‚
â”‚    â”œâ”€ agregar_metadata_administrativa()           â”‚
â”‚    â”œâ”€ merge_images_deployments()                  â”‚
â”‚    â”œâ”€ merge_with_projects()                       â”‚
â”‚    â”œâ”€ asignar_corporacion_geografica()            â”‚
â”‚    â””â”€ calcular_deployment_days()                  â”‚
â”‚                                                   â”‚
â”‚  generate_parquets.py                             â”‚
â”‚    â”œâ”€ generar_observations_parquet()              â”‚
â”‚    â”œâ”€ generar_deployments_parquet()               â”‚
â”‚    â”œâ”€ generar_projects_parquet()                  â”‚
â”‚    â””â”€ generar_todas_las_tablas()                  â”‚
â”‚                                                   â”‚
â”‚  validation.py                                    â”‚
â”‚    â”œâ”€ validar_observations_parquet()              â”‚
â”‚    â”œâ”€ validar_deployments_parquet()               â”‚
â”‚    â”œâ”€ validar_projects_parquet()                  â”‚
â”‚    â””â”€ generar_reporte_calidad()                   â”‚
â”‚                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Flujo de Procesamiento

### Diagrama de Flujo Completo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 0: PREPARACIÃ“N DEL ENTORNO                             â”‚
â”‚   â€¢ Limpieza de carpeta de salida                           â”‚
â”‚   â€¢ EliminaciÃ³n de archivos Parquet previos                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 1: CARGA DE DATOS CRUDOS                               â”‚
â”‚   1.1 Cargar projects.csv                                   â”‚
â”‚   1.2 Cargar deployments.csv                                â”‚
â”‚   1.3 Concatenar images_*.csv (mÃºltiples archivos)          â”‚
â”‚       â†’ concatenar_archivos_csv()                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 2: FILTRADO Y LIMPIEZA                                 â”‚
â”‚   2.1 Procesar timestamps                                   â”‚
â”‚       â†’ procesar_timestamps()                               â”‚
â”‚   2.2 Limpiar registros Computer Vision (CV)                â”‚
â”‚       â†’ limpiar_registros_cv()                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 3: ENRIQUECIMIENTO Y TRANSFORMACIONES                  â”‚
â”‚   3.1 Crear nombre cientÃ­fico (sp_binomial)                 â”‚
â”‚       â†’ crear_nombre_cientifico()                           â”‚
â”‚   3.2 Agregar metadata administrativa                       â”‚
â”‚       â†’ agregar_metadata_administrativa()                   â”‚
â”‚   3.3 Merge imÃ¡genes + deployments                          â”‚
â”‚       â†’ merge_images_deployments()                          â”‚
â”‚   3.4 Filtrar por subproject_name vÃ¡lido (YYYY_N)           â”‚
â”‚       â†’ filtrar_por_subproject_valido()                     â”‚
â”‚   3.5 Merge con projects                                    â”‚
â”‚       â†’ merge_with_projects()                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 4: ANÃLISIS GEOGRÃFICO                                 â”‚
â”‚   4.1 Asignar Corporaciones por coordenadas                 â”‚
â”‚       â†’ asignar_corporacion_geografica()                    â”‚
â”‚         â”œâ”€ Cargar shapefile CAR_MPIO.shp                    â”‚
â”‚         â”œâ”€ Crear puntos geomÃ©tricos (lat, lon)              â”‚
â”‚         â”œâ”€ Spatial join (point-in-polygon)                  â”‚
â”‚         â””â”€ Asignar NOMBRE_CAR a cada deployment             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 4.5: PREPARACIÃ“N FINAL DE DATOS                        â”‚
â”‚   4.5.1 Calcular deployment_days                            â”‚
â”‚         â†’ calcular_deployment_days()                        â”‚
â”‚   4.5.2 Diagnosticar columnas duplicadas                    â”‚
â”‚   4.5.3 Crear columnas de fecha (photo_date, hour)          â”‚
â”‚   4.5.4 Verificar subproject_name                           â”‚
â”‚   4.5.5 Validar columnas esenciales                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 5: GENERACIÃ“N DE ARCHIVOS PARQUET                      â”‚
â”‚   5.1 Generar observations.parquet (20 columnas)            â”‚
â”‚       â†’ generar_observations_parquet()                      â”‚
â”‚   5.2 Generar deployments.parquet (15 columnas)             â”‚
â”‚       â†’ generar_deployments_parquet()                       â”‚
â”‚   5.3 Generar projects.parquet (10 columnas)                â”‚
â”‚       â†’ generar_projects_parquet()                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ FASE 6: VALIDACIÃ“N DE CALIDAD                               â”‚
â”‚   6.1 Validar observations.parquet                          â”‚
â”‚       â†’ validar_observations_parquet()                      â”‚
â”‚   6.2 Validar deployments.parquet                           â”‚
â”‚       â†’ validar_deployments_parquet()                       â”‚
â”‚   6.3 Validar projects.parquet                              â”‚
â”‚       â†’ validar_projects_parquet()                          â”‚
â”‚   6.4 Generar reporte consolidado                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SALIDA: 3 ARCHIVOS PARQUET OPTIMIZADOS                      â”‚
â”‚   â€¢ observations.parquet   (500 KB - 5 MB)                  â”‚
â”‚   â€¢ deployments.parquet    (50 KB - 200 KB)                 â”‚
â”‚   â€¢ projects.parquet       (10 KB - 50 KB)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ MÃ³dulos del Pipeline

### 1. `src/utils.py` - Utilidades de Carga y Filtrado

**Funciones principales:**

#### `concatenar_archivos_csv(folder_path, patron='images')`

Concatena mÃºltiples archivos CSV de imÃ¡genes de Wildlife Insights.

**ParÃ¡metros:**
- `folder_path` (str): Ruta a carpeta con archivos CSV
- `patron` (str): PatrÃ³n de bÃºsqueda (default: 'images')

**Retorna:**
- `pd.DataFrame`: DataFrame consolidado con todas las imÃ¡genes

**Proceso:**
1. Buscar archivos que coincidan con `images_*.csv`
2. Leer cada archivo con `low_memory=False`
3. Concatenar verticalmente con `pd.concat()`
4. Resetear Ã­ndices

**Ejemplo:**
```python
images = concatenar_archivos_csv(
    folder_path='../1_Data_RAW_WI',
    patron='images'
)
# Output: DataFrame con ~250,000 registros
```

#### `procesar_timestamps(df)`

Convierte columnas de fecha/hora a formato datetime estÃ¡ndar.

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame con columnas de tiempo

**Retorna:**
- `pd.DataFrame`: DataFrame con timestamps procesados

**Columnas procesadas:**
- `photo_datetime` â†’ `datetime64[ns]`
- `deployment_start` â†’ `datetime64[ns]`
- `deployment_end` â†’ `datetime64[ns]`

**Manejo de errores:**
- Valores invÃ¡lidos â†’ `pd.NaT`
- Formatos mÃºltiples â†’ inferencia automÃ¡tica

#### `filtrar_por_subproject_valido(df)`

Filtra registros por eventos de muestreo vÃ¡lidos (formato YYYY_N, aÃ±o â‰¥ 2024).

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame con columna `subproject_name`

**Retorna:**
- `pd.DataFrame`: DataFrame filtrado

**Criterios de validaciÃ³n:**
```python
# Formato vÃ¡lido: YYYY_N
# Ejemplos vÃ¡lidos:
#   - 2024_1
#   - 2024_2
#   - 2025_1
#
# Rechazados:
#   - 2023_1 (aÃ±o < 2024)
#   - 2024 (sin sufijo _N)
#   - test_1 (no numÃ©rico)
#   - vacÃ­o o NaN
```

**Proceso:**
1. Filtrar no nulos
2. Verificar longitud == 6 caracteres
3. Extraer aÃ±o (primeros 4 caracteres)
4. Validar aÃ±o >= 2024
5. Verificar formato YYYY_N con regex

#### `limpiar_registros_cv(df)`

Elimina registros generados por Computer Vision (identificaciones automÃ¡ticas).

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame con columna `identified_by`

**Retorna:**
- `pd.DataFrame`: DataFrame sin registros CV

**Criterio de filtrado:**
```python
# Eliminar registros donde:
df = df[df['identified_by'] != 'Machine']
```

---

### 2. `src/transformations.py` - Transformaciones y Enriquecimiento

#### `crear_nombre_cientifico(df)`

Crea columna `sp_binomial` combinando gÃ©nero y especie.

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame con columnas `genus` y `species`

**Retorna:**
- `pd.DataFrame`: DataFrame con `sp_binomial` agregada

**LÃ³gica:**
```python
# Caso 1: GÃ©nero y especie presentes
genus = "Panthera", species = "onca"
â†’ sp_binomial = "Panthera onca"

# Caso 2: Solo gÃ©nero
genus = "Panthera", species = NaN
â†’ sp_binomial = "Panthera sp."

# Caso 3: Ambos vacÃ­os
genus = NaN, species = NaN
â†’ sp_binomial = "Unknown"
```

#### `agregar_metadata_administrativa(df, admin_name, organization)`

Agrega columnas de metadata administrativa.

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame de observaciones
- `admin_name` (str): Nombre del administrador
- `organization` (str): OrganizaciÃ³n responsable

**Retorna:**
- `pd.DataFrame`: DataFrame con columnas agregadas

**Columnas creadas:**
```python
df['admin_name'] = admin_name
df['organization'] = organization
df['processing_date'] = pd.Timestamp.now()
```

#### `merge_images_deployments(images_df, deployments_df)`

Fusiona datos de imÃ¡genes con metadata de deployments.

**ParÃ¡metros:**
- `images_df` (pd.DataFrame): Observaciones de fauna
- `deployments_df` (pd.DataFrame): ConfiguraciÃ³n de cÃ¡maras

**Retorna:**
- `pd.DataFrame`: DataFrame fusionado

**Columnas clave del merge:**
```python
# Key: deployment_id
# Type: left join (conservar todas las imÃ¡genes)
# Columnas agregadas:
#   - latitude, longitude (coordenadas)
#   - placename (nombre del sitio)
#   - deployment_start, deployment_end (fechas)
#   - subproject_name (evento de muestreo)
```

#### `merge_with_projects(data_df, projects_df)`

Fusiona datos con catÃ¡logo de proyectos.

**ParÃ¡metros:**
- `data_df` (pd.DataFrame): Observaciones enriquecidas
- `projects_df` (pd.DataFrame): CatÃ¡logo de proyectos

**Retorna:**
- `pd.DataFrame`: DataFrame con metadata de proyecto

**Columnas agregadas:**
```python
# Key: project_id
# Columnas:
#   - project_name (nombre del proyecto)
#   - project_admin (administrador)
#   - project_country (paÃ­s)
```

#### `asignar_corporacion_geografica(deployments_df, shapefile_path)`

Asigna Corporaciones AutÃ³nomas Regionales (CARs) por anÃ¡lisis geoespacial.

**ParÃ¡metros:**
- `deployments_df` (pd.DataFrame): Deployments con coordenadas
- `shapefile_path` (str): Ruta al shapefile de CARs

**Retorna:**
- `pd.DataFrame`: Tabla con `project_id` y `Corporacion`

**Proceso:**
```python
1. Cargar shapefile con geopandas
   â†’ car_gdf = gpd.read_file(shapefile_path)

2. Crear GeoDataFrame de deployments
   â†’ geometry = [Point(lon, lat) for lat, lon in coords]
   â†’ deployments_gdf = gpd.GeoDataFrame(deployments_df, geometry=geometry, crs='EPSG:4326')

3. Spatial join (point-in-polygon)
   â†’ result = gpd.sjoin(deployments_gdf, car_gdf, how='left', predicate='within')

4. Agrupar por project_id (mayorÃ­a de deployments)
   â†’ corporacion = result.groupby('project_id')['NOMBRE_CAR'].agg(lambda x: x.mode()[0])

5. Retornar tabla project_id â†’ Corporacion
```

**Manejo de casos especiales:**
- Deployments sin coordenadas â†’ `Corporacion = NaN`
- Punto fuera de polÃ­gonos â†’ `Corporacion = "Sin asignar"`
- MÃºltiples CARs en proyecto â†’ Seleccionar moda (mÃ¡s frecuente)

#### `calcular_deployment_days(df)`

Calcula dÃ­as de funcionamiento de cada deployment.

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame con `deployment_start` y `deployment_end`

**Retorna:**
- `pd.DataFrame`: DataFrame con columna `deployment_days`

**FÃ³rmula:**
```python
deployment_days = (deployment_end - deployment_start).dt.days
```

---

### 3. `src/generate_parquets.py` - GeneraciÃ³n de Archivos Parquet

#### `generar_observations_parquet(df, output_path)`

Genera archivo Parquet de observaciones con 20 columnas seleccionadas.

**ParÃ¡metros:**
- `df` (pd.DataFrame): DataFrame completo de observaciones
- `output_path` (str): Ruta de salida

**Retorna:**
- `bool`: True si exitoso, False si error

**Columnas incluidas (20):**
```python
columnas_observations = [
    'project_id',          # ID del proyecto
    'project_name',        # Nombre del proyecto
    'Corporacion',         # CAR asignada geogrÃ¡ficamente
    'subproject_name',     # Evento de muestreo (YYYY_N)
    'deployment_name',     # ID del deployment
    'placename',           # Nombre del sitio
    'latitude',            # Coordenada latitud
    'longitude',           # Coordenada longitud
    'sp_binomial',         # Nombre cientÃ­fico (Genus species)
    'genus',               # GÃ©nero taxonÃ³mico
    'species',             # EpÃ­teto especÃ­fico
    'class',               # Clase taxonÃ³mica (Mammalia, Aves)
    'common_name',         # Nombre comÃºn
    'photo_datetime',      # Timestamp de la fotografÃ­a
    'photo_date',          # Fecha (YYYY-MM-DD)
    'hour',                # Hora del dÃ­a (0-23)
    'deployment_days',     # DÃ­as de funcionamiento del deployment
    'admin_name',          # Administrador del proyecto
    'organization',        # OrganizaciÃ³n responsable
    'identified_by'        # QuiÃ©n identificÃ³ (Human/Machine)
]
```

**ConfiguraciÃ³n de compresiÃ³n:**
```python
df[columnas_observations].to_parquet(
    output_path,
    engine='pyarrow',
    compression='snappy',  # Balance velocidad/tamaÃ±o
    index=False
)
```

#### `generar_deployments_parquet(df, output_path)`

Genera archivo Parquet de deployments (15 columnas).

**Columnas incluidas:**
```python
columnas_deployments = [
    'deployment_id',
    'deployment_name',
    'project_id',
    'Corporacion',
    'subproject_name',
    'placename',
    'latitude',
    'longitude',
    'deployment_start',
    'deployment_end',
    'deployment_days',
    'camera_id',
    'feature_type',
    'bait',
    'quiet_period'
]
```

#### `generar_projects_parquet(df, output_path)`

Genera archivo Parquet de proyectos (10 columnas).

**Columnas incluidas:**
```python
columnas_projects = [
    'project_id',
    'project_name',
    'project_admin',
    'project_country',
    'metadata_license',
    'embargo',
    'observation_license',
    'sensor_height',
    'sensor_orientation',
    'detection_distance'
]
```

#### `generar_todas_las_tablas(observations_df, deployments_df, projects_df, output_dir)`

FunciÃ³n orquestadora que genera las 3 tablas simultÃ¡neamente.

**ParÃ¡metros:**
- `observations_df` (pd.DataFrame): Observaciones enriquecidas
- `deployments_df` (pd.DataFrame): Deployments procesados
- `projects_df` (pd.DataFrame): CatÃ¡logo de proyectos
- `output_dir` (str): Directorio de salida

**Retorna:**
- `bool`: True si todas exitosas, False si alguna falla

**Proceso:**
```python
1. Verificar/crear directorio de salida
2. Generar observations.parquet
3. Generar deployments.parquet
4. Generar projects.parquet
5. Verificar tamaÃ±os de archivos
6. Reportar estadÃ­sticas
```

---

### 4. `src/validation.py` - ValidaciÃ³n de Calidad

#### `validar_observations_parquet(parquet_path)`

Valida estructura y contenido de observations.parquet.

**Verificaciones:**
```python
âœ“ Archivo existe
âœ“ Formato Parquet vÃ¡lido
âœ“ NÃºmero de registros > 0
âœ“ Columnas esperadas presentes (20 columnas)
âœ“ Tipos de datos correctos
âœ“ Valores nulos en columnas crÃ­ticas < 5%
âœ“ Rango de fechas vÃ¡lido (> 2020)
âœ“ Coordenadas en rango Colombia (-5 < lat < 13, -80 < lon < -66)
```

**Output de consola:**
```
âœ“ observations.parquet cargado: 175,000 registros
âœ“ Columnas presentes: 20/20
âœ“ Tipos de datos correctos
âš  Columna 'common_name' tiene 2.3% valores nulos (aceptable)
âœ“ Rango de fechas: 2024-01-15 a 2025-12-09
âœ“ Coordenadas vÃ¡lidas (100% dentro de Colombia)
```

#### `validar_deployments_parquet(parquet_path)`

Valida deployments.parquet.

**Verificaciones:**
```python
âœ“ Archivo existe
âœ“ NÃºmero de deployments Ãºnico
âœ“ Coordenadas vÃ¡lidas
âœ“ Fechas start < end
âœ“ deployment_days > 0
âœ“ No hay duplicados de deployment_id
```

#### `validar_projects_parquet(parquet_path)`

Valida projects.parquet.

**Verificaciones:**
```python
âœ“ Archivo existe
âœ“ project_id Ãºnico (sin duplicados)
âœ“ Columnas obligatorias presentes
âœ“ Licencias vÃ¡lidas
```

#### `generar_reporte_calidad(observations_path, deployments_path, projects_path)`

Genera reporte consolidado de calidad de datos.

**Output:**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
REPORTE DE CALIDAD DE DATOS - WILDLIFE INSIGHTS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

1. OBSERVATIONS.PARQUET
   â€¢ Registros totales: 175,000
   â€¢ Especies Ãºnicas: 87
   â€¢ Proyectos: 12
   â€¢ Eventos: 8
   â€¢ Rango temporal: 2024-01-15 a 2025-12-09
   â€¢ Completitud: 97.8%

2. DEPLOYMENTS.PARQUET
   â€¢ Deployments totales: 1,200
   â€¢ Proyectos: 12
   â€¢ DuraciÃ³n promedio: 45 dÃ­as
   â€¢ Coordenadas vÃ¡lidas: 100%

3. PROJECTS.PARQUET
   â€¢ Proyectos totales: 12
   â€¢ Corporaciones: 8
   â€¢ PaÃ­ses: 1 (Colombia)

VALIDACIÃ“N GENERAL: âœ“ APROBADO
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ“Š Formatos de Datos

### Entrada: Archivos CSV de Wildlife Insights

#### `projects.csv`

**Estructura:**
```csv
project_id,project_name,project_admin,project_country,metadata_license,...
2002517,Proyecto CAM Norte,Juan PÃ©rez,Colombia,CC0,...
2008342,Fototrampeo CORPOCALDAS,Ana LÃ³pez,Colombia,CC-BY,...
```

**Columnas clave:**
- `project_id` (int): Identificador Ãºnico del proyecto
- `project_name` (str): Nombre descriptivo
- `project_admin` (str): Responsable administrativo
- `project_country` (str): PaÃ­s de origen

#### `deployments.csv`

**Estructura:**
```csv
deployment_id,deployment_name,project_id,placename,latitude,longitude,deployment_start,deployment_end,...
d001,CAM_Site001,2002517,Bosque La Pradera,4.6542,-74.1234,2024-03-15,2024-05-20,...
d002,CAM_Site002,2002517,RÃ­o Verde,4.7123,-74.0987,2024-03-16,2024-05-21,...
```

**Columnas clave:**
- `deployment_id` (str): ID Ãºnico del deployment
- `deployment_name` (str): Nombre del sitio
- `latitude`, `longitude` (float): Coordenadas WGS84
- `deployment_start`, `deployment_end` (str): Fechas ISO 8601

#### `images_*.csv` (mÃºltiples archivos)

**Estructura:**
```csv
project_id,deployment_id,genus,species,common_name,photo_datetime,identified_by,...
2002517,d001,Panthera,onca,Jaguar,2024-04-12 14:35:22,Human,...
2002517,d001,Tapirus,terrestris,Danta,2024-04-13 08:12:45,Human,...
```

**Columnas clave:**
- `project_id` (int): Vincula con projects.csv
- `deployment_id` (str): Vincula con deployments.csv
- `genus`, `species` (str): TaxonomÃ­a
- `common_name` (str): Nombre comÃºn
- `photo_datetime` (str): Timestamp de captura
- `identified_by` (str): Human/Machine

### Salida: Archivos Parquet Optimizados

#### `observations.parquet`

**Esquema:**
```
project_id: int64
project_name: string
Corporacion: string
subproject_name: string
deployment_name: string
placename: string
latitude: float64
longitude: float64
sp_binomial: string
genus: string
species: string
class: string
common_name: string
photo_datetime: datetime64[ns]
photo_date: date32
hour: int8
deployment_days: int16
admin_name: string
organization: string
identified_by: string
```

**TamaÃ±o tÃ­pico:** 500 KB - 5 MB  
**CompresiÃ³n:** Snappy (~70% reducciÃ³n vs CSV)

#### `deployments.parquet`

**Esquema:**
```
deployment_id: string
deployment_name: string
project_id: int64
Corporacion: string
subproject_name: string
placename: string
latitude: float64
longitude: float64
deployment_start: datetime64[ns]
deployment_end: datetime64[ns]
deployment_days: int16
camera_id: string
feature_type: string
bait: string
quiet_period: int16
```

**TamaÃ±o tÃ­pico:** 50 KB - 200 KB

#### `projects.parquet`

**Esquema:**
```
project_id: int64
project_name: string
project_admin: string
project_country: string
metadata_license: string
embargo: bool
observation_license: string
sensor_height: float32
sensor_orientation: string
detection_distance: float32
```

**TamaÃ±o tÃ­pico:** 10 KB - 50 KB

---

## âœ… ValidaciÃ³n y Control de Calidad

### Criterios de ValidaciÃ³n

#### 1. ValidaciÃ³n de Formato `subproject_name`

**Regla:** Debe tener formato `YYYY_N` donde YYYY â‰¥ 2024

**Ejemplos vÃ¡lidos:**
- `2024_1` âœ“
- `2024_2` âœ“
- `2025_1` âœ“

**Ejemplos rechazados:**
- `2023_1` âœ— (aÃ±o < 2024)
- `2024` âœ— (falta sufijo _N)
- `test_1` âœ— (no numÃ©rico)
- ` ` âœ— (vacÃ­o)

**CÃ³digo de validaciÃ³n:**
```python
def es_subproject_valido(subproject_name):
    if pd.isna(subproject_name) or len(str(subproject_name)) != 6:
        return False
    
    try:
        year = int(str(subproject_name)[:4])
        return year >= 2024
    except:
        return False
```

#### 2. ValidaciÃ³n de Coordenadas

**Rango vÃ¡lido para Colombia:**
- Latitud: -5Â° a 13Â° N
- Longitud: -80Â° a -66Â° W

**CÃ³digo:**
```python
def coordenadas_validas(lat, lon):
    return (-5 <= lat <= 13) and (-80 <= lon <= -66)
```

#### 3. ValidaciÃ³n de Fechas

**Criterios:**
- `deployment_start` < `deployment_end`
- Fechas > 2020-01-01
- `photo_datetime` entre `deployment_start` y `deployment_end` (con tolerancia)

#### 4. ValidaciÃ³n de Completitud

**Columnas crÃ­ticas (< 5% nulos permitido):**
- `project_id`
- `deployment_name`
- `sp_binomial`
- `latitude`, `longitude`
- `photo_datetime`

**Columnas opcionales (> 5% nulos permitido):**
- `common_name`
- `identified_by`
- `bait`

### Reportes AutomÃ¡ticos

El pipeline genera reportes detallados al finalizar:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
VALIDACIÃ“N DE OBSERVATIONS.PARQUET
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

âœ“ Archivo cargado exitosamente
âœ“ Registros totales: 175,234
âœ“ Columnas presentes: 20/20

COMPLETITUD POR COLUMNA:
  project_id:         100.0%  âœ“
  project_name:       100.0%  âœ“
  Corporacion:         98.5%  âœ“
  sp_binomial:        100.0%  âœ“
  common_name:         97.2%  âœ“
  latitude:           100.0%  âœ“
  longitude:          100.0%  âœ“
  photo_datetime:     100.0%  âœ“

DISTRIBUCIÃ“N DE DATOS:
  Especies Ãºnicas:             87
  Proyectos:                   12
  Eventos (subproject_name):    8
  Deployments:              1,200
  
RANGO TEMPORAL:
  Fecha mÃ­nima:  2024-01-15
  Fecha mÃ¡xima:  2025-12-09
  DÃ­as totales:  329

COORDENADAS:
  Rango latitud:   1.23Â° a 11.45Â°  âœ“
  Rango longitud: -77.89Â° a -68.12Â° âœ“
  Fuera de Colombia: 0 registros  âœ“

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Variables de ConfiguraciÃ³n en `process_RAW_data_WI.py`

```python
# Rutas base
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)
BASE_RAW_PATH = os.path.join(PROJECT_ROOT, '1_Data_RAW_WI')
BASE_OUTPUT_PATH = os.path.join(PROJECT_ROOT, '4_Dashboard', 'dashboard_input_data')
SHAPEFILE_PATH = os.path.join(PROJECT_ROOT, '2_Data_Shapefiles_CARs', 'CAR_MPIO.shp')
```

**Modificar para estructura diferente:**
```python
# Ejemplo: Carpeta de salida personalizada
BASE_OUTPUT_PATH = os.path.join(PROJECT_ROOT, 'output', 'parquet_files')
```

### ParÃ¡metros de CompresiÃ³n Parquet

```python
# En src/generate_parquets.py
df.to_parquet(
    output_path,
    engine='pyarrow',
    compression='snappy',  # Opciones: 'snappy', 'gzip', 'brotli', 'zstd'
    index=False
)
```

**ComparaciÃ³n de compresiones:**

| Algoritmo | TamaÃ±o | Velocidad Escritura | Velocidad Lectura | Uso Recomendado |
|-----------|--------|---------------------|-------------------|-----------------|
| `snappy` | ~70% reducciÃ³n | âš¡âš¡âš¡ Muy rÃ¡pida | âš¡âš¡âš¡ Muy rÃ¡pida | **Dashboard interactivo** âœ“ |
| `gzip` | ~80% reducciÃ³n | âš¡âš¡ Moderada | âš¡âš¡ Moderada | Archivado a largo plazo |
| `brotli` | ~85% reducciÃ³n | âš¡ Lenta | âš¡âš¡ Moderada | CompresiÃ³n mÃ¡xima |
| `zstd` | ~75% reducciÃ³n | âš¡âš¡âš¡ RÃ¡pida | âš¡âš¡âš¡ RÃ¡pida | Balance Ã³ptimo |

**RecomendaciÃ³n:** Mantener `snappy` para dashboards (velocidad > tamaÃ±o).

### ConfiguraciÃ³n de Metadata Administrativa

```python
# En FASE 3.2 de process_RAW_data_WI.py
images = agregar_metadata_administrativa(
    images,
    admin_name="Cristian Acevedo",      # Personalizar
    organization="Instituto Humboldt"   # Personalizar
)
```

---

## ğŸš€ OptimizaciÃ³n y Rendimiento

### Benchmarks de Rendimiento

**Hardware de prueba:**
- CPU: Intel i7-10750H (6 cores)
- RAM: 16 GB
- SSD: NVMe PCIe 3.0

**Tiempos de ejecuciÃ³n:**

| TamaÃ±o de Datos | Tiempo Total | Fase MÃ¡s Lenta |
|-----------------|--------------|----------------|
| 50,000 registros | ~15 segundos | AnÃ¡lisis geogrÃ¡fico (5s) |
| 175,000 registros | ~45 segundos | AnÃ¡lisis geogrÃ¡fico (18s) |
| 500,000 registros | ~2.5 minutos | AnÃ¡lisis geogrÃ¡fico (1m) |

### Optimizaciones Implementadas

#### 1. Lectura Eficiente de CSV

```python
# Antes (lento):
images = pd.read_csv('images_large.csv')

# DespuÃ©s (optimizado):
images = pd.read_csv('images_large.csv', low_memory=False)
```

#### 2. ConcatenaciÃ³n de DataFrames

```python
# Antes (ineficiente - mÃºltiples appends):
result = pd.DataFrame()
for file in files:
    df = pd.read_csv(file)
    result = result.append(df)

# DespuÃ©s (eficiente - una sola concatenaciÃ³n):
dfs = [pd.read_csv(file) for file in files]
result = pd.concat(dfs, ignore_index=True)
```

#### 3. Filtrado Temprano

```python
# OptimizaciÃ³n: Filtrar datos innecesarios antes de merge
images = limpiar_registros_cv(images)  # Reducir tamaÃ±o antes de merge
data = merge_images_deployments(images, deployments)  # Merge mÃ¡s rÃ¡pido
```

#### 4. Spatial Join Optimizado

```python
# En asignar_corporacion_geografica()
# Usar Ã­ndice espacial implÃ­cito de geopandas
result = gpd.sjoin(deployments_gdf, car_gdf, how='left', predicate='within')
# GeoPandas usa R-tree index automÃ¡ticamente
```

### Recomendaciones para Datasets Grandes (> 1M registros)

**1. Procesamiento en Chunks:**

```python
# Modificar concatenar_archivos_csv() para lectura en chunks
chunk_size = 100000
chunks = []
for file in image_files:
    for chunk in pd.read_csv(file, chunksize=chunk_size):
        # Filtrar inmediatamente
        chunk = chunk[chunk['subproject_name'].notna()]
        chunks.append(chunk)

images = pd.concat(chunks, ignore_index=True)
```

**2. Usar Dask para ParalelizaciÃ³n:**

```python
import dask.dataframe as dd

# Lectura paralela de mÃºltiples CSVs
ddf = dd.read_csv('../1_Data_RAW_WI/images_*.csv')

# Procesamiento paralelo
ddf = ddf[ddf['identified_by'] != 'Machine']
ddf = ddf.compute()  # Convertir a pandas al final
```

**3. Reducir Uso de Memoria:**

```python
# Especificar tipos de datos eficientes
dtypes = {
    'project_id': 'int32',  # En lugar de int64
    'deployment_name': 'category',  # En lugar de object
    'class': 'category',
    'genus': 'category'
}

images = pd.read_csv('images.csv', dtype=dtypes)
```

---

## ğŸ”§ ResoluciÃ³n de Problemas

### Errores Comunes

#### Error: `FileNotFoundError: CAR_MPIO.shp not found`

**Causa:** Shapefile no estÃ¡ en la ubicaciÃ³n esperada.

**SoluciÃ³n:**
```bash
# Verificar que existen todos los archivos
ls -l 2_Data_Shapefiles_CARs/CAR_MPIO.*

# Debe mostrar:
# CAR_MPIO.shp
# CAR_MPIO.shx
# CAR_MPIO.dbf
# CAR_MPIO.prj
```

Si falta alguno, descargar shapefile completo.

#### Error: `KeyError: 'subproject_name'`

**Causa:** Columna `subproject_name` no existe en deployments.csv.

**SoluciÃ³n:**
1. Verificar que Wildlife Insights exportÃ³ el campo correctamente
2. Si no existe, crear columna temporal:
   ```python
   deployments['subproject_name'] = deployments['project_id'].astype(str) + '_1'
   ```

#### Error: `ValueError: No objects to concatenate`

**Causa:** No se encontraron archivos `images_*.csv`.

**SoluciÃ³n:**
```python
# Verificar archivos en carpeta
import glob
files = glob.glob('../1_Data_RAW_WI/images_*.csv')
print(f"Archivos encontrados: {len(files)}")

# Si len(files) == 0:
# - Verificar nombre de archivos (debe empezar con "images_")
# - Verificar extensiÃ³n (.csv, no .CSV o .txt)
```

#### Error: `MemoryError` durante procesamiento

**Causa:** Dataset muy grande para RAM disponible.

**SoluciÃ³n temporal:**
```python
# 1. Reducir tamaÃ±o de chunk en lectura
chunk_size = 50000  # Reducir de 100000

# 2. Liberar memoria despuÃ©s de cada fase
import gc
gc.collect()

# 3. Procesar proyectos individualmente
for project_id in projects['project_id'].unique():
    data_proyecto = images[images['project_id'] == project_id]
    # Procesar y guardar por separado
```

#### Warning: `DtypeWarning: Columns have mixed types`

**Causa:** Pandas infiere tipos incorrectamente.

**SoluciÃ³n:**
```python
# Especificar tipos explÃ­citamente
images = pd.read_csv(
    file,
    dtype={
        'project_id': 'int',
        'deployment_id': 'str',
        'genus': 'str',
        'species': 'str'
    },
    low_memory=False
)
```

### Debugging del Pipeline

**Activar modo verbose:**

```python
# En process_RAW_data_WI.py, agregar despuÃ©s de imports:
import logging
logging.basicConfig(level=logging.DEBUG)

# Las funciones imprimirÃ¡n informaciÃ³n detallada
```

**Inspeccionar datos intermedios:**

```python
# DespuÃ©s de cada fase, agregar:
print(f"\n=== DEBUG: FASE X ===")
print(f"Columnas: {list(data.columns)}")
print(f"Tipos: {data.dtypes}")
print(f"Registros: {len(data)}")
print(f"Valores nulos:\n{data.isnull().sum()}")
print(data.head())
```

**Guardar checkpoints:**

```python
# DespuÃ©s de fases crÃ­ticas, guardar CSV intermedio
data.to_csv('checkpoint_fase3.csv', index=False)

# Recuperar en caso de error:
data = pd.read_csv('checkpoint_fase3.csv')
```

---

## ğŸ“ Soporte

Para problemas no resueltos en esta documentaciÃ³n:

1. **Revisar Issues:** https://github.com/[USUARIO]/Dashboard_Monitoreo_Camaras_Trampa_Red_OTUS_Colombia/issues
2. **Crear nuevo Issue** con:
   - VersiÃ³n de Python y librerÃ­as (`pip list`)
   - Mensaje de error completo
   - TamaÃ±o aproximado del dataset
   - Sistema operativo

---

<div align="center">

**Ãšltima actualizaciÃ³n:** Enero 2025  
**VersiÃ³n del pipeline:** 3.0 (Arquitectura Modular)

</div>
