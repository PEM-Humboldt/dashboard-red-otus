"""
Pipeline de procesamiento de datos de c√°maras trampa desde Wildlife Insights.

Este m√≥dulo orquesta el procesamiento de datos crudos de Wildlife Insights y genera
archivos Parquet optimizados para visualizaci√≥n en un dashboard de R/Shiny.

Arquitectura Modular:
    - src.utils: Utilidades de carga y filtrado de datos
    - src.transformations: Transformaciones y enriquecimiento de datos
    - src.generate_parquets: Generaci√≥n de archivos Parquet
    - src.validation: Validaci√≥n de calidad de datos

Flujo de procesamiento:
    1. Carga de datos crudos (proyectos, despliegues, im√°genes)
    2. Filtrado y validaci√≥n (subproject_name, registros CV)
    3. Enriquecimiento (nombres cient√≠ficos, metadata administrativa)
    4. An√°lisis geogr√°fico (asignaci√≥n de CARs)
    5. Generaci√≥n de 3 tablas Parquet:
       - observations.parquet: Datos granulares de observaciones
       - deployments.parquet: Metadata de despliegues
       - projects.parquet: Cat√°logo de proyectos con estad√≠sticas
    6. Validaci√≥n de calidad de datos

Entrada:
    - 1_Data_RAW_WI/projects.csv: Informaci√≥n maestra de proyectos
    - 1_Data_RAW_WI/deployments.csv: Despliegues de c√°maras
    - 1_Data_RAW_WI/images_*.csv: Im√°genes por proyecto (m√∫ltiples archivos)
    - 2_Data_Shapefiles_CARs/CAR_MPIO.shp: Shapefile de CARs

Salida:
    - observations.parquet: 20 columnas, datos de observaciones con filtros
    - deployments.parquet: 15 columnas, metadata de despliegues
    - projects.parquet: 10 columnas, cat√°logo de proyectos

Validaci√≥n de proyectos:
    - subproject_name debe tener exactamente 6 caracteres (formato: YYYY_N)
    - A√±o >= 2020
    - Se excluyen: eventos vac√≠os, formato incorrecto, a√±os antiguos

Author: Proyecto OTUS - Instituto Humboldt
Version: 3.0 (Arquitectura Modular)
Date: Enero 2025
"""

import os
import sys
import pandas as pd
from pathlib import Path

# Configurar encoding UTF-8 para Windows
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

# Importar m√≥dulos del pipeline
from src.utils import (
    concatenar_archivos_csv,
    procesar_timestamps,
    filtrar_por_subproject_valido,
    filtrar_fechas_inconsistentes,
    limpiar_registros_cv
)
from src.transformations import (
    crear_nombre_cientifico,
    agregar_metadata_administrativa,
    merge_images_deployments,
    merge_with_projects,
    asignar_corporacion_geografica
)
from src.generate_parquets import generar_todas_las_tablas
from src.validation import (
    validar_observations_parquet,
    validar_deployments_parquet,
    validar_projects_parquet,
    generar_reporte_calidad
)

# ===============================================================================
# CONFIGURACI√ìN GLOBAL
# ===============================================================================

# Rutas base (relativas a la ubicaci√≥n de este script)
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(SCRIPT_DIR)  # Directorio ra√≠z del proyecto (un nivel arriba)
BASE_RAW_PATH = os.path.join(PROJECT_ROOT, '1_Data_RAW_WI')
BASE_OUTPUT_PATH = os.path.join(PROJECT_ROOT, '4_Dashboard', 'dashboard_input_data')
SHAPEFILE_PATH = os.path.join(PROJECT_ROOT, '2_Data_Shapefiles_CARs', 'CAR_MPIO.shp')

# ===============================================================================
# FUNCI√ìN PRINCIPAL DE PROCESAMIENTO
# ===============================================================================

def main():
    """
    Funci√≥n principal que ejecuta el flujo completo de procesamiento modular.
    
    Flujo:
        1. Carga de datos crudos (proyectos, despliegues, im√°genes)
        2. Filtrado y limpieza (subproject_name, CV)
        3. Enriquecimiento y transformaciones
        4. An√°lisis geogr√°fico
        5. Generaci√≥n de Parquets
        6. Validaci√≥n de calidad
    """
    print("="*80)
    print("PIPELINE DE PROCESAMIENTO DE DATOS - WILDLIFE INSIGHTS")
    print("Arquitectura Modular - Generaci√≥n de 3 tablas Parquet")
    print("="*80)
    
    # ===========================================================================
    # FASE 0: LIMPIEZA DE CARPETA DE SALIDA
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 0: PREPARACI√ìN DEL ENTORNO")
    print("="*80)
    
    print("\n0.1 Limpiando carpeta de salida...")
    # Crear directorio si no existe
    if not os.path.exists(BASE_OUTPUT_PATH):
        os.makedirs(BASE_OUTPUT_PATH)
        print(f"  ‚úì Directorio creado: {BASE_OUTPUT_PATH}")
    else:
        # Limpiar archivos .parquet existentes
        archivos_eliminados = 0
        for archivo in os.listdir(BASE_OUTPUT_PATH):
            if archivo.endswith('.parquet'):
                archivo_path = os.path.join(BASE_OUTPUT_PATH, archivo)
                try:
                    os.remove(archivo_path)
                    archivos_eliminados += 1
                except Exception as e:
                    print(f"  ‚ö† No se pudo eliminar {archivo}: {e}")
        
        if archivos_eliminados > 0:
            print(f"  ‚úì {archivos_eliminados} archivo(s) Parquet eliminado(s)")
        else:
            print(f"  ‚úì Carpeta de salida vac√≠a")
    
    # ===========================================================================
    # FASE 1: CARGA DE DATOS CRUDOS
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 1: CARGA DE DATOS CRUDOS")
    print("="*80)
    
    # 1.1 Cargar archivos de referencia
    print("\n1.1 Cargando archivos de referencia...")
    projects_path = os.path.join(BASE_RAW_PATH, 'projects.csv')
    deployments_path = os.path.join(BASE_RAW_PATH, 'deployments.csv')
    
    try:
        projects = pd.read_csv(projects_path)
        deployments = pd.read_csv(deployments_path)
        print(f"  ‚úì projects.csv: {len(projects):,} proyectos")
        print(f"  ‚úì deployments.csv: {len(deployments):,} despliegues")
    except Exception as e:
        print(f"  ‚úó Error al cargar archivos de referencia: {e}")
        return False
    
    # 1.2 Cargar y concatenar im√°genes
    print("\n1.2 Cargando archivos de im√°genes...")
    images = concatenar_archivos_csv(
        folder_path=BASE_RAW_PATH,
        patron='images'
    )
    
    if images is None or images.empty:
        print("  ‚úó No se pudieron cargar im√°genes")
        return False
    
    print(f"  ‚úì Im√°genes cargadas: {len(images):,} registros")
    
    # ===========================================================================
    # FASE 2: FILTRADO Y LIMPIEZA
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 2: FILTRADO Y LIMPIEZA")
    print("="*80)
    
    # 2.1 Procesar timestamps
    print("\n2.1 Procesando timestamps...")
    images = procesar_timestamps(images)
    
    # 2.2 Limpiar registros CV
    print("\n2.2 Limpiando registros de Computer Vision...")
    registros_antes = len(images)
    images = limpiar_registros_cv(images)
    registros_despues = len(images)
    print(f"  ‚úì Registros despu√©s de limpieza CV: {registros_despues:,} de {registros_antes:,}")
    
    # ===========================================================================
    # FASE 3: ENRIQUECIMIENTO Y TRANSFORMACIONES
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 3: ENRIQUECIMIENTO Y TRANSFORMACIONES")
    print("="*80)
    
    # 3.1 Crear nombre cient√≠fico
    print("\n3.1 Creando nombres cient√≠ficos...")
    images = crear_nombre_cientifico(images)
    
    # 3.2 Agregar metadata administrativa
    print("\n3.2 Agregando metadata administrativa...")
    images = agregar_metadata_administrativa(
        images,
        admin_name="",
        organization="Instituto Humboldt"
    )
    
    # 3.3 Merge con deployments
    print("\n3.3 Fusionando im√°genes con deployments...")
    data = merge_images_deployments(images, deployments)
    print(f"  ‚úì Registros despu√©s del merge: {len(data):,}")
    
    # 3.4 Filtrar por subproject_name v√°lido (DESPU√âS del merge)
    print("\n3.4 Filtrando por subproject_name v√°lido (YYYY_N, a√±o >= 2020)...")
    registros_antes = len(data)
    data = filtrar_por_subproject_valido(data)
    registros_despues = len(data)
    print(f"  ‚úì Registros filtrados: {registros_despues:,} de {registros_antes:,}")
    
    if data.empty:
        print("  ‚úó No hay registros v√°lidos despu√©s del filtrado")
        return False
    
    # 3.5 Filtrar fechas inconsistentes con evento (ej: 2025_1 con datos de 2019)
    print("\n3.5 Filtrando fechas inconsistentes con evento...")
    registros_antes = len(data)
    data = filtrar_fechas_inconsistentes(data, columna_fecha='photo_datetime', columna_subproject='subproject_name')
    registros_despues = len(data)
    print(f"  ‚úì Registros filtrados: {registros_despues:,} de {registros_antes:,}")
    
    if data.empty:
        print("  ‚úó No hay registros v√°lidos despu√©s del filtrado de fechas")
        return False
    
    # 3.6 Merge con projects
    print("\n3.6 Fusionando con informaci√≥n de proyectos...")
    data = merge_with_projects(data, projects)
    print(f"  ‚úì Registros finales: {len(data):,}")
    
    # ===========================================================================
    # FASE 4: AN√ÅLISIS GEOGR√ÅFICO
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 4: AN√ÅLISIS GEOGR√ÅFICO")
    print("="*80)
    
    # 4.1 Asignar Corporaciones geogr√°ficamente
    print("\n4.1 Asignando Corporaciones por an√°lisis geogr√°fico...")
    # La funci√≥n retorna corporaciones a nivel de proyecto (project_id, Corporacion)
    project_corporaciones = asignar_corporacion_geografica(
        deployments_df=deployments,
        shapefile_path=SHAPEFILE_PATH
    )
    
    # Hacer merge por project_id para agregar la columna Corporacion
    data = data.merge(
        project_corporaciones[['project_id', 'Corporacion']],
        on='project_id',
        how='left'
    )
    
    corporaciones_asignadas = data['Corporacion'].notna().sum()
    print(f"  ‚úì Corporaciones asignadas: {corporaciones_asignadas:,} observaciones")
    
    # ===========================================================================
    # FASE 4.5: PREPARACI√ìN FINAL DE DATOS
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 4.5: PREPARACI√ìN FINAL DE DATOS")
    print("="*80)
    
    # 4.5.1 Calcular deployment_days
    print("\n4.5.1 Calculando deployment_days...")
    from src.transformations import calcular_deployment_days
    data = calcular_deployment_days(data)
    
    # 4.5.2 Diagnosticar columnas duplicadas
    print("\n4.5.2 Diagnosticando columnas duplicadas...")
    duplicated_cols = data.columns[data.columns.duplicated()].tolist()
    if duplicated_cols:
        print(f"  ‚ö† COLUMNAS DUPLICADAS DETECTADAS: {duplicated_cols}")
        print(f"  Total de columnas: {len(data.columns)}")
        print(f"  Columnas √∫nicas: {data.columns.nunique()}")
        # Eliminar columnas duplicadas qued√°ndonos con la primera ocurrencia
        data = data.loc[:, ~data.columns.duplicated()]
        print(f"  ‚úì Duplicados eliminados. Columnas restantes: {len(data.columns)}")
    else:
        print(f"  ‚úì No hay columnas duplicadas ({len(data.columns)} columnas)")
    
    # 4.5.3 Crear columnas de fecha adicionales
    print("\n4.5.3 Creando columnas de fecha adicionales...")
    if 'photo_datetime' in data.columns:
        data['photo_date'] = pd.to_datetime(data['photo_datetime']).dt.date
        if 'hour' not in data.columns:
            data['hour'] = pd.to_datetime(data['photo_datetime']).dt.hour
        print(f"  ‚úì Columnas de fecha creadas")
    
    # 4.5.4 Asegurar que subproject_name existe
    if 'subproject_name' not in data.columns:
        print("\n4.5.4 Agregando columna subproject_name...")
        # Intentar obtenerla de deployments si no existe
        if 'subproject_name' in deployments.columns:
            # Ya deber√≠a estar del merge, pero por si acaso
            print("  ‚ö† subproject_name faltante, revisar merge anterior")
        data['subproject_name'] = ''
        print(f"  ‚ö† Columna subproject_name creada vac√≠a")
    
    # 4.5.5 Verificar columnas esenciales
    print("\n4.5.5 Verificando columnas esenciales...")
    columnas_esenciales = [
        'project_id', 'project_name', 'Corporacion', 'deployment_name',
        'placename', 'latitude', 'longitude', 'sp_binomial', 'genus',
        'species', 'class', 'photo_datetime'
    ]
    
    columnas_presentes = [col for col in columnas_esenciales if col in data.columns]
    columnas_faltantes = [col for col in columnas_esenciales if col not in data.columns]
    
    print(f"  ‚úì Columnas presentes: {len(columnas_presentes)}/{len(columnas_esenciales)}")
    if columnas_faltantes:
        print(f"  ‚ö† Columnas faltantes: {columnas_faltantes}")
    
    # ===========================================================================
    # FASE 5: GENERACI√ìN DE ARCHIVOS PARQUET
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 5: GENERACI√ìN DE ARCHIVOS PARQUET")
    print("="*80)
    
    # 5.1 Generar las 3 tablas esenciales
    print("\n5.1 Generando archivos Parquet...")
    resultado = generar_todas_las_tablas(
        observations_df=data,
        deployments_df=deployments,
        projects_df=projects,
        output_dir=BASE_OUTPUT_PATH
    )
    
    if not resultado:
        print("  ‚úó Error al generar archivos Parquet")
        return False
    
    # ===========================================================================
    # FASE 6: VALIDACI√ìN DE CALIDAD
    # ===========================================================================
    print("\n" + "="*80)
    print("FASE 6: VALIDACI√ìN DE CALIDAD")
    print("="*80)
    
    # 6.1 Validar observations.parquet
    print("\n6.1 Validando observations.parquet...")
    obs_path = os.path.join(BASE_OUTPUT_PATH, 'observations.parquet')
    validar_observations_parquet(obs_path)
    
    # 6.2 Validar deployments.parquet
    print("\n6.2 Validando deployments.parquet...")
    deps_path = os.path.join(BASE_OUTPUT_PATH, 'deployments.parquet')
    validar_deployments_parquet(deps_path)
    
    # 6.3 Validar projects.parquet
    print("\n6.3 Validando projects.parquet...")
    projs_path = os.path.join(BASE_OUTPUT_PATH, 'projects.parquet')
    validar_projects_parquet(projs_path)
    
    # 6.4 Resumen de validaci√≥n
    print("\n6.4 Resumen de validaci√≥n completado")
    print("  ‚úì Validaciones ejecutadas para las 3 tablas Parquet")
    print("  ‚ö† Revise los detalles arriba para identificar columnas faltantes")
    
    # ===========================================================================
    # RESUMEN FINAL
    # ===========================================================================
    print("\n" + "="*80)
    print("PROCESAMIENTO COMPLETADO EXITOSAMENTE")
    print("="*80)
    
    print(f"\nüìä RESUMEN:")
    print(f"  ‚Ä¢ Total observaciones: {len(data):,}")
    print(f"  ‚Ä¢ Especies √∫nicas: {data['sp_binomial'].nunique()}")
    print(f"  ‚Ä¢ Proyectos: {data['project_id'].nunique()}")
    print(f"  ‚Ä¢ Eventos (subproject_name): {data['subproject_name'].nunique()}")
    print(f"  ‚Ä¢ Despliegues: {data['deployment_name'].nunique()}")
    
    print(f"\nüìÅ ARCHIVOS GENERADOS:")
    print(f"  ‚Ä¢ observations.parquet: Datos granulares de observaciones")
    print(f"  ‚Ä¢ deployments.parquet: Metadata de despliegues")
    print(f"  ‚Ä¢ projects.parquet: Cat√°logo de proyectos")
    
    print(f"\n‚ú® Pipeline ejecutado correctamente con arquitectura modular")
    
    return True


# ===============================================================================
# SCRIPT PRINCIPAL
# ===============================================================================

if __name__ == "__main__":
    main()
